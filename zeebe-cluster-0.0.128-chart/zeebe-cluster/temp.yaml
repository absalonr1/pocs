NAME: zeebe-cluster
LAST DEPLOYED: Tue Sep 15 12:29:47 2020
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
elasticsearch:
  antiAffinity: soft

COMPUTED VALUES:
JavaOpts: -XX:MaxRAMPercentage=25.0 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data
  -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -XX:+ExitOnOutOfMemoryError
affinity: {}
clusterSize: "3"
cpuThreadCount: "2"
elasticsearch:
  antiAffinity: soft
  antiAffinityTopologyKey: kubernetes.io/hostname
  clusterHealthCheckParams: wait_for_status=green&timeout=1s
  clusterName: elasticsearch
  enabled: true
  esConfig: {}
  esJavaOpts: -Xmx1g -Xms1g
  esMajorVersion: ""
  extraEnvs: []
  extraInitContainers: ""
  extraVolumeMounts: ""
  extraVolumes: ""
  fsGroup: ""
  fullnameOverride: ""
  global:
    elasticsearch:
      host: elasticsearch-master
      port: 9200
    zeebe: '{{ .Release.Name }}-zeebe'
  httpPort: 9200
  image: docker.elastic.co/elasticsearch/elasticsearch
  imagePullPolicy: IfNotPresent
  imagePullSecrets: []
  imageTag: 6.8.5
  ingress:
    annotations: {}
    enabled: false
    hosts:
    - chart-example.local
    path: /
    tls: []
  initResources: {}
  keystore: []
  labels: {}
  lifecycle: {}
  masterService: ""
  masterTerminationFix: false
  maxUnavailable: 1
  minimumMasterNodes: 2
  nameOverride: ""
  networkHost: 0.0.0.0
  nodeAffinity: {}
  nodeGroup: master
  nodeSelector: {}
  persistence:
    annotations: {}
    enabled: true
  podAnnotations: {}
  podManagementPolicy: Parallel
  podSecurityContext:
    fsGroup: 1000
    runAsUser: 1000
  podSecurityPolicy:
    create: false
    name: ""
    spec:
      fsGroup:
        rule: RunAsAny
      privileged: true
      runAsUser:
        rule: RunAsAny
      seLinux:
        rule: RunAsAny
      supplementalGroups:
        rule: RunAsAny
      volumes:
      - secret
      - configMap
      - persistentVolumeClaim
  priorityClassName: ""
  protocol: http
  rbac:
    create: false
    serviceAccountName: ""
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5
  replicas: 3
  resources:
    limits:
      cpu: 1000m
      memory: 2Gi
    requests:
      cpu: 100m
      memory: 2Gi
  roles:
    data: "true"
    ingest: "true"
    master: "true"
  schedulerName: ""
  secretMounts: []
  securityContext:
    capabilities:
      drop:
      - ALL
    runAsNonRoot: true
    runAsUser: 1000
  service:
    annotations: {}
    httpPortName: http
    labels: {}
    labelsHeadless: {}
    nodePort: ""
    transportPortName: transport
    type: ClusterIP
  sidecarResources: {}
  sysctlInitContainer:
    enabled: true
  sysctlVmMaxMapCount: 262144
  terminationGracePeriod: 120
  tolerations: []
  transportPort: 9300
  updateStrategy: RollingUpdate
  volumeClaimTemplate:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 30Gi
env: []
gateway:
  env: []
  logLevel: info
  podDisruptionBudget:
    enabled: false
    maxUnavailable: null
    minAvailable: 1
  replicas: 1
global:
  elasticsearch:
    host: elasticsearch-master
    port: 9200
  zeebe: '{{ .Release.Name }}-zeebe'
image:
  pullPolicy: IfNotPresent
  repository: camunda/zeebe
  tag: 0.23.4
ioThreadCount: "2"
kibana:
  affinity: {}
  elasticsearchHosts: http://elasticsearch-master:9200
  elasticsearchURL: ""
  enabled: false
  extraEnvs: []
  fullnameOverride: ""
  global:
    elasticsearch:
      host: elasticsearch-master
      port: 9200
    zeebe: '{{ .Release.Name }}-zeebe'
  healthCheckPath: /app/kibana
  httpPort: 5601
  image: docker.elastic.co/kibana/kibana
  imagePullPolicy: IfNotPresent
  imagePullSecrets: []
  imageTag: 6.8.5
  ingress:
    annotations: {}
    enabled: false
    hosts:
    - chart-example.local
    path: /
    tls: []
  kibanaConfig: {}
  labels: {}
  lifecycle: {}
  maxUnavailable: 1
  nameOverride: ""
  nodeSelector: {}
  podAnnotations: {}
  podSecurityContext:
    fsGroup: 1000
  priorityClassName: ""
  protocol: http
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5
  replicas: 1
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 500Mi
  secretMounts: []
  securityContext:
    capabilities:
      drop:
      - ALL
    runAsNonRoot: true
    runAsUser: 1000
  serverHost: 0.0.0.0
  service:
    annotations: {}
    labels: {}
    nodePort: ""
    port: 5601
    type: ClusterIP
  serviceAccount: ""
  tolerations: []
  updateStrategy:
    type: Recreate
labels:
  app: zeebe
logLevel: info
nodeSelector: {}
partitionCount: "3"
podDisruptionBudget:
  enabled: false
  maxUnavailable: 1
  minAvailable: null
probePath: /ready
prometheus:
  enabled: false
  servicemonitor:
    enabled: false
pvcAccessModes:
- ReadWriteOnce
pvcSize: 10Gi
readinessProbe:
  failureThreshold: 1
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 1
replicationFactor: "3"
resources:
  limits:
    cpu: 1000m
    memory: 4Gi
  requests:
    cpu: 500m
    memory: 2Gi
serviceCommandPort: 26501
serviceGatewayPort: 26500
serviceHttpPort: 9600
serviceInternalPort: 26502
serviceType: ClusterIP
tolerations: []

HOOKS:
---
# Source: zeebe-cluster/charts/elasticsearch/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "zeebe-cluster-nxjxa-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
  - name: "zeebe-cluster-ogrpu-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:6.8.5"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
---
# Source: zeebe-cluster/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "zeebe-cluster-zeebe-test-connection"
  labels:
    app.kubernetes.io/name: zeebe-cluster
    app.kubernetes.io/instance: zeebe-cluster
    helm.sh/chart: zeebe-cluster-0.0.128
    app.kubernetes.io/version: "0.23.4"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args:  ['zeebe-cluster-zeebe:9600']
  restartPolicy: Never
MANIFEST:
---
# Source: zeebe-cluster/charts/elasticsearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: "elasticsearch-master-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "elasticsearch-master"
---
# Source: zeebe-cluster/templates/configmap.yaml
kind: ConfigMap
metadata:
  name: zeebe-cluster
  labels:
    app.kubernetes.io/name: zeebe-cluster
    app.kubernetes.io/instance: zeebe-cluster
    helm.sh/chart: zeebe-cluster-0.0.128
    app.kubernetes.io/version: "0.23.4"
    app.kubernetes.io/managed-by: Helm
apiVersion: v1
data:
  startup.sh: |
    #!/usr/bin/env bash
    set -eux -o pipefail

    export ZEEBE_BROKER_NETWORK_ADVERTISEDHOST=${ZEEBE_BROKER_NETWORK_ADVERTISEDHOST:-$(hostname -f)}
    export ZEEBE_BROKER_CLUSTER_NODEID=${ZEEBE_BROKER_CLUSTER_NODEID:-${K8S_POD_NAME##*-}}

    # As the number of replicas or the DNS is not obtainable from the downward API yet,
    # defined them here based on conventions
    export ZEEBE_BROKER_CLUSTER_CLUSTERSIZE=${ZEEBE_BROKER_CLUSTER_CLUSTERSIZE:-1}
    contactPointPrefix=${K8S_POD_NAME%-*}
    contactPoints=${ZEEBE_BROKER_CLUSTER_INITIALCONTACTPOINTS:-""}
    if [[ -z "${contactPoints}" ]]; then
      for ((i=0; i<${ZEEBE_BROKER_CLUSTER_CLUSTERSIZE}; i++))
      do
        contactPoints="${contactPoints},${contactPointPrefix}-$i.$(hostname -d):26502"
      done

      export ZEEBE_BROKER_CLUSTER_INITIALCONTACTPOINTS="${contactPoints}"
    fi
    
    if [ "$(ls -A /exporters/)" ]; then
      mkdir /usr/local/zeebe/exporters/
      cp -a /exporters/*.jar /usr/local/zeebe/exporters/
    else  
      echo "No exporters available."
    fi

    exec /usr/local/zeebe/bin/broker

  application.yaml: |

  broker-log4j2.xml: |

  gateway-log4j2.xml: |
---
# Source: zeebe-cluster/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "zeebe-cluster"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    heritage: "Helm"
    release: "zeebe-cluster"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: zeebe-cluster/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master-headless
  labels:
    heritage: "Helm"
    release: "zeebe-cluster"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "elasticsearch-master"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: zeebe-cluster/templates/gateway-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: "zeebe-cluster-zeebe-gateway"
  labels:
    app.kubernetes.io/name: zeebe-cluster
    app.kubernetes.io/instance: zeebe-cluster
    helm.sh/chart: zeebe-cluster-0.0.128
    app.kubernetes.io/version: "0.23.4"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: gateway
  annotations:
    null
spec:
  selector:
      app.kubernetes.io/name: zeebe-cluster
      app.kubernetes.io/instance: zeebe-cluster
      helm.sh/chart: zeebe-cluster-0.0.128
      app.kubernetes.io/version: "0.23.4"
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: gateway
  ports:
    - port: 9600
      protocol: TCP
      name: http
    - port: 26500
      protocol: TCP
      name: gateway
---
# Source: zeebe-cluster/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: "zeebe-cluster-zeebe"
  labels:
    app.kubernetes.io/name: zeebe-cluster
    app.kubernetes.io/instance: zeebe-cluster
    helm.sh/chart: zeebe-cluster-0.0.128
    app.kubernetes.io/version: "0.23.4"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: broker
    app: zeebe
  annotations:
    null    
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
    - port: 9600
      protocol: TCP
      name: http  
    - port: 26502
      protocol: TCP
      name: internal
    - port: 26501
      protocol: TCP
      name: command
  selector:
    app.kubernetes.io/name: zeebe-cluster
    app.kubernetes.io/instance: zeebe-cluster
    helm.sh/chart: zeebe-cluster-0.0.128
    app.kubernetes.io/version: "0.23.4"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: broker
---
# Source: zeebe-cluster/templates/gateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "zeebe-cluster-zeebe-gateway"
  labels:
    app.kubernetes.io/name: zeebe-cluster
    app.kubernetes.io/instance: zeebe-cluster
    helm.sh/chart: zeebe-cluster-0.0.128
    app.kubernetes.io/version: "0.23.4"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: gateway
  annotations:
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: zeebe-cluster
      app.kubernetes.io/instance: zeebe-cluster
      helm.sh/chart: zeebe-cluster-0.0.128
      app.kubernetes.io/version: "0.23.4"
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: gateway
  template:
    metadata:
      labels:
        app.kubernetes.io/name: zeebe-cluster
        app.kubernetes.io/instance: zeebe-cluster
        helm.sh/chart: zeebe-cluster-0.0.128
        app.kubernetes.io/version: "0.23.4"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: gateway
      annotations:
    spec:
      containers:
        - name: zeebe-cluster
          image: "camunda/zeebe:0.23.4"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9600
              name: http
            - containerPort: 26500
              name: gateway
            - containerPort: 26502
              name: internal
          env:
            - name: ZEEBE_STANDALONE_GATEWAY
              value: "true"
            - name: ZEEBE_GATEWAY_CLUSTER_CLUSTERNAME
              value: zeebe-cluster-zeebe
            - name: ZEEBE_GATEWAY_CLUSTER_MEMBERID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ZEEBE_LOG_LEVEL
              value: "info"
            - name: JAVA_TOOL_OPTIONS
              value: "-XX:MaxRAMPercentage=25.0 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -XX:+ExitOnOutOfMemoryError"
            - name: ZEEBE_GATEWAY_CLUSTER_CONTACTPOINT
              value: zeebe-cluster-zeebe:26502
            - name: ZEEBE_GATEWAY_NETWORK_HOST
              value: 0.0.0.0
            - name: ZEEBE_GATEWAY_NETWORK_PORT
              value: "26500"
            - name: ZEEBE_GATEWAY_CLUSTER_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ZEEBE_GATEWAY_CLUSTER_PORT
              value: "26502"
            - name: ZEEBE_GATEWAY_MONITORING_HOST
              value: 0.0.0.0
            - name: ZEEBE_GATEWAY_MONITORING_PORT
              value: "9600"
          volumeMounts:
          securityContext:
            null
          readinessProbe:
            tcpSocket:
              port: gateway
            initialDelaySeconds: 20
            periodSeconds: 5
      volumes:
        - name: config
          configMap:
            name: zeebe-cluster
            defaultMode: 0744
---
# Source: zeebe-cluster/charts/elasticsearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "zeebe-cluster"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    esMajorVersion: "6"
spec:
  serviceName: elasticsearch-master-headless
  selector:
    matchLabels:
      app: "elasticsearch-master"
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-master
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 30Gi
  template:
    metadata:
      name: "elasticsearch-master"
      labels:
        heritage: "Helm"
        release: "zeebe-cluster"
        chart: "elasticsearch"
        app: "elasticsearch-master"
        app: "elasticsearch-master"
      annotations:
        
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "elasticsearch-master"
      terminationGracePeriodSeconds: 120
      volumes:
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:6.8.5"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:6.8.5"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: 'wait_for_status=green&timeout=1s' )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                http () {
                    local path="${1}"
                    if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                      BASIC_AUTH="-u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                    else
                      BASIC_AUTH=''
                    fi
                    curl -XGET -s -k --fail ${BASIC_AUTH} http://127.0.0.1:9200${path}
                }

                if [ -f "${START_FILE}" ]; then
                    echo 'Elasticsearch is already running, lets check the node is healthy and there are master nodes available'
                    http "/_cluster/health?timeout=0s"
                else
                    echo 'Waiting for elasticsearch cluster to become cluster to be ready (request params: "wait_for_status=green&timeout=1s" )'
                    if http "/_cluster/health?wait_for_status=green&timeout=1s" ; then
                        touch ${START_FILE}
                        exit 0
                    else
                        echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                        exit 1
                    fi
                fi
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 100m
            memory: 2Gi
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.zen.minimum_master_nodes
            value: "2"
          - name: discovery.zen.ping.unicast.hosts
            value: "elasticsearch-master-headless"
          - name: cluster.name
            value: "elasticsearch"
          - name: network.host
            value: "0.0.0.0"
          - name: ES_JAVA_OPTS
            value: "-Xmx1g -Xms1g"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "true"
        volumeMounts:
          - name: "elasticsearch-master"
            mountPath: /usr/share/elasticsearch/data
---
# Source: zeebe-cluster/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "zeebe-cluster-zeebe"
  labels:
    app.kubernetes.io/name: zeebe-cluster
    app.kubernetes.io/instance: zeebe-cluster
    helm.sh/chart: zeebe-cluster-0.0.128
    app.kubernetes.io/version: "0.23.4"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: broker
    app: zeebe
  annotations:   
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: zeebe-cluster
      app.kubernetes.io/instance: zeebe-cluster
      helm.sh/chart: zeebe-cluster-0.0.128
      app.kubernetes.io/version: "0.23.4"
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: broker
  serviceName: "zeebe-cluster-zeebe"
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app.kubernetes.io/name: zeebe-cluster
        app.kubernetes.io/instance: zeebe-cluster
        helm.sh/chart: zeebe-cluster-0.0.128
        app.kubernetes.io/version: "0.23.4"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: broker
      annotations:   
    spec:
      initContainers:    
      containers:
      - name: zeebe-cluster
        image: "camunda/zeebe:0.23.4"
        imagePullPolicy: IfNotPresent
        env:
        - name: ZEEBE_BROKER_CLUSTER_CLUSTERNAME
          value: zeebe-cluster-zeebe
        - name: ZEEBE_LOG_LEVEL
          value: 
        - name: ZEEBE_BROKER_CLUSTER_PARTITIONSCOUNT
          value: "3"
        - name: ZEEBE_BROKER_CLUSTER_CLUSTERSIZE
          value: "3"
        - name: ZEEBE_BROKER_CLUSTER_REPLICATIONFACTOR
          value: "3"
        - name: ZEEBE_BROKER_THREADS_CPUTHREADCOUNT
          value: "2"
        - name: ZEEBE_BROKER_THREADS_IOTHREADCOUNT
          value: "2"
        - name: ZEEBE_BROKER_GATEWAY_ENABLE
          value: "false"
        - name: ZEEBE_BROKER_EXPORTERS_ELASTICSEARCH_CLASSNAME
          value: "io.zeebe.exporter.ElasticsearchExporter"
        - name: ZEEBE_BROKER_EXPORTERS_ELASTICSEARCH_ARGS_URL
          value: "http://elasticsearch-master:9200"
        - name: ZEEBE_BROKER_NETWORK_COMMANDAPI_PORT
          value: "26501"
        - name: ZEEBE_BROKER_NETWORK_INTERNALAPI_PORT
          value: "26502"
        - name: ZEEBE_BROKER_NETWORK_MONITORINGAPI_PORT
          value: "9600"         
        - name: K8S_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name              
        - name: JAVA_TOOL_OPTIONS
          value: "-XX:MaxRAMPercentage=25.0 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -XX:+ExitOnOutOfMemoryError"
        ports:
        - containerPort: 9600
          name: http
        - containerPort: 26501
          name: command
        - containerPort: 26502
          name: internal
        readinessProbe:
          httpGet:
            path: /ready
            port: 9600
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
            limits:
              cpu: 1000m
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
        volumeMounts:
        - name: config
          mountPath: /usr/local/zeebe/config/application.yaml
          subPath: application.yaml
        - name: config
          mountPath: /usr/local/bin/startup.sh
          subPath: startup.sh
        - name: data
          mountPath: /usr/local/zeebe/data
        - name: exporters
          mountPath: /exporters
        securityContext:
          null
      volumes:
      - name: config
        configMap:
          name: zeebe-cluster
          defaultMode: 0744
      - name: exporters
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ReadWriteOnce]
      storageClassName: 
      resources:
        requests:
          storage: "10Gi"

NOTES:
______     ______     ______     ______     ______    
/\___  \   /\  ___\   /\  ___\   /\  == \   /\  ___\   
\/_/  /__  \ \  __\   \ \  __\   \ \  __<   \ \  __\   
  /\_____\  \ \_____\  \ \_____\  \ \_____\  \ \_____\ 
  \/_____/   \/_____/   \/_____/   \/_____/   \/_____/                                                                                                                                      

(zeebe-cluster - 0.0.128)

- Docker Image used for Broker: camunda/zeebe:0.23.4
- Cluster Name: "zeebe-cluster-zeebe"
- ElasticSearch Enabled: true - URL: http://elasticsearch-master:9200
- Kibana Enabled: false
- Prometheus Enabled: false
- Prometheus ServiceMonitor Enabled: false

The Cluster itself is not exposed as a service that means that you can use kubectl port-forward to access the Zeebe cluster from outside Kubernetes:

> kubectl port-forward svc/zeebe-cluster-zeebe-gateway 26500:26500 -n default

Now you can connect your workers and clients to `localhost:26500`
